# Genetic Algorithm Testing #
To test the code which we wrote for the Genetic Algorithms, we decided to use one of the twenty-one algorithms as a basis for the tests. The reason for this was that the algorithm which we chose to use, the nineteenth genetic algorithm, contained every method that any of the other twenty-one algorithms made use of. There was nothing new in any of the other genetic algorithms, they just used a different combination of methods and had different sized genotypes. This meant that after testing everything we could in the nineteenth algorithm, we had essentially tested every genetic algorithm.<br> 

We made use of Eclipse and JUnit in order to write test code for this genetic algorithm. Basically, we took one class at a time and wrote test methods which ensured that each method contained in that class functioned as expected and produced correct outputs. 

### Unit Testing ###
The first class that we unit tested was the BaseClass, which contained all the methods for subtracting and adding amounts from the fighter’s total based on data it received from text files. These unit tests were relatively straightforward. We made a genotype and then made variables for each piece of data that could be received. Then, using a BaseClass object, we would assert that the total returned by each method was the correct total. Below is a screenshot of the test we ran on the calcReach method, which passed.<br>

![](http://www.student.computing.dcu.ie/~nearyc2/UnitTestingBaseClassScreenshot.JPG)

The rest of the tests in this class are much the same as seen above, and they all passed.<br>
The next class we unit tested only contained one method. This method was very important to the success of the genetic algorithm because it used the genotypes produced by the genetic algorithm to attempt to predict a fight, and returned whether or not it predicted the fight correctly using the values contained in the genotype. So to test this, we used variables to simulate the input coming from a text file. Then coded a genotype into a variable. Using these variables we ran the calculateWinner method and ensured that the method returned a value of 1, indicating that the method predicted the correct winner of the fight. This method can be seen below.<br>

![](http://www.student.computing.dcu.ie/~nearyc2/UnitTestingAlgoClassScreenshot.JPG)

We then unit tested the class which we had been using to test fitness of genotypes against unseen fights, to give us an idea of how well a genotype performed when generalised to fights that it had not learned from. This test only involved writing one method which ran the testOnUnseenFights method using a genotype that we already knew should return a value of 24. As you can see in the following screenshot, we just checked to see it returned the correct value, which it did.

![](http://www.student.computing.dcu.ie/~nearyc2/UnitTestingAlgoTesterScreenshot.JPG)

The next class to be unit tested was the Individual class which basically contained the genes of an individual and methods to retrieve them and manipulate them. The first method we tested was the method to create an individual, we did this by checking an Individual object’s genes before and after the call to the createIndividual method – before the call its genes should be empty, but after, the genes should contain values. The rest of the methods in this class were getters and setters, and we tested these by coding the inputs and such using variables and then ensuring that the correct values were retrieved by the methods. Some of these methods can be seen below.

![](http://www.student.computing.dcu.ie/~nearyc2/UnitTestingIndividualClassScreenshot.JPG)

The final class that could be simply unit tested was the CalculateFitness class, which only contained one method that returned the fitness of an individual based on that individual’s genotype. Again, we used genes that we knew should return a result of 29.

![](http://www.student.computing.dcu.ie/~nearyc2/UnitTestingCalculateFitnessScreenshot.JPG)

### Integration Testing ###
For the following classes, the lines between unit test and integration test became slightly blurred, because these classes made use of other classes in their methods, so we are going to describe them under the heading of integration tests.<br>
The Chromosome class contained methods to manipulate the genes of Individuals in a Population, using evolutionary methods such as crossover, mutation and natural selection. We tested the performCrossover method by calling it on an empty Individual object and then checking that the object was not empty after the method call. The naturalSelection method was tested by creating a Population object and then calling the naturalSelection method with the Population object passed as a parameter and placing the result into a previously empty Individual object. To see if it worked, we ensured that the Individual object was no longer empty. The performEvolution method was tested in almost the exact same manner as the naturalSelection method, except it returned a Population object instead of an Individual object. The most awkward method to test was the performMutation method, which has a probability of 0.005 of changing a gene of an Individual. To test that this worked, we created two Individuals with identical genotypes. We then tried to mutate the second individual’s genotype by calling the performMutation method. We looped until it changed and then ensured that it had worked by comparing the two genotypes and asserting that they were not equal.
![](http://www.student.computing.dcu.ie/~nearyc2/IntegrationTestingChromosomeScreenshot.JPG)

The final class to be tested was the Population class, which is responsible for building and manipulating populations of Individuals. We tested getting and setting of Individual objects in this class in the same manner as before. We also tested the constructors because they took a Boolean as a parameter, which signalled whether or not all the Individuals in the Population should be given random genes or not, because this is needed for the first generation of a genetic algorithm. To test this we called the constructor with false as a parameter and then ensured that attempting to access an individual in the Population threw an exception. Then when we called it with true as a parameter we could access an individual in the Population with no exception being thrown. The most awkward method to test in this class was the getFittestInPopulation method. To test that this worked as expected, we created a population of individuals, all of whom had genes with a value of zero (because this gives a horrible fitness). We then changed one Individual in the population to have genes which we know to have a very high fitness. We then called the method on the population and ensured that it returned the individual with the best genotype as expected. This test can be seen below.
![](http://www.student.computing.dcu.ie/~nearyc2/IntegrationTestingPopulationScreenshot.JPG)

### System Testing ###
The system test for this genetic algorithm basically consisted of us running it over and over again and seeing that it did indeed evolve and improve in its ability to predict the fight. We wrote a main method which basically created a random population of individuals and then, using mutation, crossover and natural selection, this population was replaced with a new and improved population, which formed the next generation. This cycle continued until the genetic algorithm stopped evolving and the best solution had been found. A screenshot of this process in a terminal can be seen below.

![](http://www.student.computing.dcu.ie/~nearyc2/SystemTestGeneticAlgorithmScreenshot.JPG)

### Results ###
Using a tool for Eclipse known as EclEmma, we were able to measure the coverage of the genetic algorithm after performing all of the tests that we had written. We achieved a total coverage score of 92.5%. We would have been closer to 100% if we had been able to write some JUnit tests for the main method of the genetic algorithm, however, this is very awkward, and rather unnecessary. The main method consisted purely of methods that we have fully tested as well as some print statements, and the fact that it evolves when run and becomes better at predicting fights as it runs, is proof enough that the main method is doing everything it should be. A screenshot showing our code coverage results can be seen below.

![](http://www.student.computing.dcu.ie/~nearyc2/GeneticAlgorithmCodeCoverageScreenshot.JPG)

# Android Application Testing #
To test the code we wrote for our Android Application, we used unit tests and instrumentation tests. Unit tests, in terms of Android applications, are tests that can be run locally, without needing a device or emulator. Whereas, instrumentation tests are tests which require a device or emulator in order to be performed. These tests mainly focus on the User Interface and actual functionality of the Application itself. Instrumentation tests can be used to describe both integration tests as well as unit tests when it comes to Android Applications, which makes some of these tests quite difficult to categorise. So for the purpose of this write-up, the tests described under the heading of unit tests, solely consist of unit tests, however, the tests described under the heading of instrumentation tests can consist of both unit tests and integration tests.<br>
Unit tests were carried out using JUnit and instrumentation tests were carried out using a combination of Espresso and JUnit.

### Unit Testing ###
It was only possible to properly unit test three of the classes in the Application. The first of which was the GlobalClass, which we used to store variables that needed to be passed between different screens on the application using getters and setters. The tests for these are rather self-explanatory, basically just test the getter by retrieving a value and asserting that it is the intended value, and test the setter by changing the value using the setter and then asserting that the value has changed. The only other method in this class was resetAllValues, which we used to reset values that may have been manipulated during a previous run of the application. This essentially tidied up everything before a new run of the application began. We tested this by manually changing all these values, then calling the method, and ensuring that the values had indeed reset. Some of these tests can be seen below.

![](http://www.student.computing.dcu.ie/~nearyc2/AppUnitTestingGlobalClassScreenshot.JPG)

The next two classes that we did unit tests on were the ResultScreen classes that contained the methods for actually predicting a winner of the fight based on the information provided by the user. These methods were similar to some of those used in the genetic algorithm so the tests were relatively similar too. We tested the calculate winner method for when the user chose to give opinion, when the user chose to use data only and for when the user chose to build their own algorithm. The tests for these methods were similar in nature. We manually entered data to simulate the user going through the application in order to receive a prediction. We then ran the method to see if it gave the result that it should have given. In all three cases it did. A screenshot from one of the tests is below.

![](http://www.student.computing.dcu.ie/~nearyc2/AppUnitTestingCalculateMethodScreenshot.JPG)

### Instrumentation Testing ###
The first class which required instrumentation testing was the DatabaseHelper class, because in order to carry out tests on the database, the database first needed to be installed on a device or an emulator. Before each test, we created a database on the testing device, and after each test we would close the database for safety. Since our database was already created, all the methods in this class are basically getters. So to test them, we would query the database at a known location and ensure that the value it retrieved was the correct value. We also had two methods to build the lists for the drop-down menu of fighter names. The first of these lists was built based on the weight division selected by the user, and the second list consisted of the same names excluding the name that the user already selected. So we tested that these methods functioned appropriately by choosing a fighter from the list and then ensuring that that fighter did not appear in the second list. Some of these methods can be seen below.

![](http://www.student.computing.dcu.ie/~nearyc2/AppInstrumentationTestingGlobalClassScreenshot.JPG)

The ChooseFighters class was another class which we performed instrumentation testing on. It consisted of drop-down menus that allow the user to select fighters based on a weight division of their choosing. We tested onCreate methods by ensuring that appropriate information was displayed on the screen when it was launched. We also tested that when the user selects a weight division, a fighter from the appropriate weight division is part of the list that is displayed to the user on the next drop-down menu. We also tested that if the user attempts to move on to the next screen before filling out all required fields, they will not be able to, and will be provided with a message asking them to fill out all required fields before moving on. Some of these methods are shown below.

![](http://www.student.computing.dcu.ie/~nearyc2/AppInstrumentationTestingChooseFightersScreenshot.JPG)

Another class which we did instrumentation testing on was the GiveOpinionOrNot class which allowed the user to choose what kind of prediction they wanted. If they decided to use statistics only, it would launch a SplashScreen animation and then display a result. To test this, we used GlobalClass to manually set the names of two fighters, to simulate the user having selected them in the previous screen. We then checked that once the animation had ended, the correct fighter was displayed as the winner to the user. The user could also choose to give their opinion as part of the prediction, in which case, the UserOpinion activity was launched. To test that this worked, we simulated the user pressing the button to give their opinion (using Espresso methods) and then ensured that some text from the appropriate activity was now being displayed to the user. Some of these methods can be seen in this screenshot.

![](http://www.student.computing.dcu.ie/~nearyc2/AppInstrumentationTestingGiveOpinionOrNotScreenshot.JPG)

The rest of the classes were mainly tested for functionality. So we used Espresso methods to simulate the user clicking all the buttons that they could on each screen and then ensuring that the correct action was performed upon the button being clicked. We also tested that the onCreate methods of these classes worked appropriately by ensuring that a piece of text or an image from the chosen screen was, in fact, being displayed to the user upon launching it. We also tested the user clicking the back buttons, either the back button on the actual device, or the back button that is on the top left of the screen. These tests were carried out on all classes and some of them can be seen below.

![](http://www.student.computing.dcu.ie/~nearyc2/AppInstrumentationTestingAllActivitiesScreenshot.JPG)

### System Testing ###
We wrote 3 separate system tests using Espresso for our Android Application. These system tests simulate a user going from the home screen all the way through to receiving a prediction. Each system test simulated the user choosing a different path through the app, by either creating their own algorithm, using our algorithm with only data, or using our algorithm as well as providing their own opinion. During these system tests we would ensure that the user was being shown the correct information at all times, and that the correct result was provided to the user on the final result screen based on the information that they had entered. A screenshot of one of these tests is provided below. The other two system tests are very similar to this one shown here.

![](http://www.student.computing.dcu.ie/~nearyc2/AppSystemTestScreenshot.JPG)

###Results###
It is somewhat harder to get a good measure of coverage when it comes to Android Applications, because of the need to do instrumentation tests to carry out most of the testing. However, we feel that having written 83 tests, consisting of unit tests, integration tests as well as system tests, that we have tested our Android Application to a very high level. We have the application on our phones for over a week now and have been trying to find problems with it, but so far we have had no crashes and have noticed no bugs.<br>
As far as testing the Application as an actual product, we used it last week to predict 10 real fights that took place in an organisation known as the Ultimate Fighting Championship. We then placed a 50 cent bet on each fighter that it predicted would be the winner. At the end of the fights, our Application had predicted 7 out of a possible 10 fights correctly, and one of the fights that it predicted incorrectly is being investigated on the basis that the judges may have made a wrong decision. So the application was very close to predicting 8 out of 10 fights correctly. It also picked 3 of the biggest underdogs on the card to win, and they all did. We placed a total of 5 euro in bets and ended up making 2.27 euro in profit, which is just over 45% profit. Considering that we used only data, and absolutely no opinion, we were very pleased with these preliminary results, and believe that our application has very good potential as a product.